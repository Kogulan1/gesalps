"""
Preprocessing Plan Executor Module
Applies preprocessing plans generated by the LLM to datasets.
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, Any, Tuple

logger = logging.getLogger(__name__)

# Try to import sklearn (optional but recommended)
try:
    from sklearn.preprocessing import QuantileTransformer, StandardScaler, MinMaxScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("scikit-learn not available - some transformations will be skipped")


def apply_preprocessing_plan(df: pd.DataFrame, plan: Dict[str, Any]) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Apply preprocessing plan to DataFrame.
    
    Args:
        df: Original DataFrame
        plan: Preprocessing plan from OpenRouter LLM
        
    Returns:
        Tuple of (preprocessed DataFrame, applied steps log)
    """
    result_df = df.copy()
    applied_steps = []
    
    try:
        # 1. Column renaming
        if "column_renames" in plan and isinstance(plan["column_renames"], dict):
            renames = {k: v for k, v in plan["column_renames"].items() if k in result_df.columns}
            if renames:
                result_df = result_df.rename(columns=renames)
                applied_steps.append(f"Renamed {len(renames)} columns: {list(renames.keys())[:3]}...")
        
        # 2. Data type corrections
        if "data_type_corrections" in plan and isinstance(plan["data_type_corrections"], dict):
            for col, dtype in plan["data_type_corrections"].items():
                if col in result_df.columns:
                    try:
                        if dtype == "int64":
                            result_df[col] = pd.to_numeric(result_df[col], errors='coerce').fillna(0).astype('int64')
                        elif dtype == "float64":
                            result_df[col] = pd.to_numeric(result_df[col], errors='coerce').astype(float)
                        elif dtype == "category":
                            result_df[col] = result_df[col].astype('category')
                        elif dtype == "boolean_to_int":
                            result_df[col] = result_df[col].astype(int).astype('int64')
                        elif dtype == "datetime_to_numeric":
                            if pd.api.types.is_datetime64_any_dtype(result_df[col]):
                                result_df[col] = pd.to_datetime(result_df[col]).astype('int64') / 1e9
                            else:
                                result_df[col] = pd.to_numeric(result_df[col], errors='coerce')
                        applied_steps.append(f"Converted '{col}' to {dtype}")
                    except Exception as e:
                        logger.warning(f"Failed to convert '{col}' to {dtype}: {e}")
        
        # 3. Datetime extractions
        if "datetime_extractions" in plan and isinstance(plan["datetime_extractions"], dict):
            for col, config in plan["datetime_extractions"].items():
                if col in result_df.columns and pd.api.types.is_datetime64_any_dtype(result_df[col]):
                    try:
                        method = config.get("method", "timestamp")
                        if method == "timestamp":
                            result_df[col] = pd.to_datetime(result_df[col]).astype('int64') / 1e9
                            applied_steps.append(f"Extracted timestamp from '{col}'")
                        elif method == "extract_features":
                            dt_col = pd.to_datetime(result_df[col])
                            features = config.get("features", ["year", "month", "day"])
                            for feature in features:
                                if feature == "year":
                                    result_df[f"{col}_year"] = dt_col.dt.year.astype('int64')
                                elif feature == "month":
                                    result_df[f"{col}_month"] = dt_col.dt.month.astype('int64')
                                elif feature == "day":
                                    result_df[f"{col}_day"] = dt_col.dt.day.astype('int64')
                                elif feature == "hour":
                                    result_df[f"{col}_hour"] = dt_col.dt.hour.astype('int64')
                                elif feature == "minute":
                                    result_df[f"{col}_minute"] = dt_col.dt.minute.astype('int64')
                            result_df = result_df.drop(columns=[col])
                            applied_steps.append(f"Extracted datetime features from '{col}': {features}")
                    except Exception as e:
                        logger.warning(f"Failed to extract datetime features from '{col}': {e}")
        
        # 4. Missing value handling
        if "missing_value_strategy" in plan and isinstance(plan["missing_value_strategy"], dict):
            strategy = plan["missing_value_strategy"]
            method = strategy.get("method")
            columns = strategy.get("columns", result_df.columns.tolist())
            
            for col in columns:
                if col in result_df.columns and result_df[col].isna().any():
                    try:
                        if method == "fill_mean" and pd.api.types.is_numeric_dtype(result_df[col]):
                            valid_values = result_df[col].dropna()
                            if len(valid_values) > 0:
                                mean_val = float(valid_values.mean())
                                result_df[col].fillna(mean_val, inplace=True)
                                applied_steps.append(f"Filled missing values in '{col}' with mean")
                        elif method == "fill_median" and pd.api.types.is_numeric_dtype(result_df[col]):
                            valid_values = result_df[col].dropna()
                            if len(valid_values) > 0:
                                median_val = float(valid_values.median())
                                result_df[col].fillna(median_val, inplace=True)
                                applied_steps.append(f"Filled missing values in '{col}' with median")
                        elif method == "fill_mode":
                            valid_values = result_df[col].dropna()
                            if len(valid_values) > 0:
                                mode_val = valid_values.mode()
                                if len(mode_val) > 0:
                                    result_df[col].fillna(mode_val.iloc[0], inplace=True)
                                    applied_steps.append(f"Filled missing values in '{col}' with mode")
                        elif method == "drop":
                            result_df = result_df.dropna(subset=[col])
                            applied_steps.append(f"Dropped rows with missing values in '{col}'")
                    except Exception as e:
                        logger.warning(f"Failed to handle missing values in '{col}': {e}")
        
        # 5. Outlier handling
        if "outlier_handling" in plan and isinstance(plan["outlier_handling"], dict):
            for col, config in plan["outlier_handling"].items():
                if col in result_df.columns and pd.api.types.is_numeric_dtype(result_df[col]):
                    try:
                        valid_values = result_df[col].dropna()
                        if len(valid_values) == 0:
                            continue
                        
                        method = config.get("method", "clip")
                        lower_p = config.get("lower_percentile", 0.01)
                        upper_p = config.get("upper_percentile", 0.99)
                        
                        if method == "clip":
                            lower_val = float(valid_values.quantile(lower_p))
                            upper_val = float(valid_values.quantile(upper_p))
                            clipped_count = ((result_df[col] < lower_val) | (result_df[col] > upper_val)).sum()
                            result_df[col] = result_df[col].clip(lower=lower_val, upper=upper_val)
                            applied_steps.append(
                                f"Clipped {clipped_count} outliers in '{col}' ({lower_p}-{upper_p} percentiles)"
                            )
                        elif method == "remove":
                            lower_val = float(valid_values.quantile(lower_p))
                            upper_val = float(valid_values.quantile(upper_p))
                            before = len(result_df)
                            result_df = result_df[(result_df[col] >= lower_val) & (result_df[col] <= upper_val)]
                            removed = before - len(result_df)
                            applied_steps.append(f"Removed {removed} rows with outliers in '{col}'")
                    except Exception as e:
                        logger.warning(f"Failed to handle outliers in '{col}': {e}")
        
        # 6. Transformations
        if "transformations" in plan and isinstance(plan["transformations"], dict):
            for col, config in plan["transformations"].items():
                if col in result_df.columns and pd.api.types.is_numeric_dtype(result_df[col]):
                    method = config.get("method")
                    
                    try:
                        if method == "quantile" and SKLEARN_AVAILABLE:
                            valid_mask = result_df[col].notna()
                            if valid_mask.sum() > 0:
                                transformer = QuantileTransformer(
                                    output_distribution='normal',
                                    n_quantiles=min(1000, valid_mask.sum())
                                )
                                result_df.loc[valid_mask, col] = transformer.fit_transform(
                                    result_df.loc[valid_mask, [col]]
                                ).flatten()
                                applied_steps.append(f"Applied quantile transformation to '{col}'")
                        elif method == "log":
                            valid_values = result_df[col].dropna()
                            if len(valid_values) > 0:
                                min_val = float(valid_values.min())
                                if min_val <= 0:
                                    result_df[col] = np.log1p(result_df[col] - min_val + 1)
                                else:
                                    result_df[col] = np.log(result_df[col].clip(lower=1e-10))
                                applied_steps.append(f"Applied log transformation to '{col}'")
                        elif method == "sqrt":
                            valid_values = result_df[col].dropna()
                            if len(valid_values) > 0:
                                min_val = float(valid_values.min())
                                if min_val < 0:
                                    result_df[col] = np.sqrt(result_df[col] - min_val)
                                else:
                                    result_df[col] = np.sqrt(result_df[col])
                                applied_steps.append(f"Applied sqrt transformation to '{col}'")
                        elif method == "standardize" and SKLEARN_AVAILABLE:
                            valid_mask = result_df[col].notna()
                            if valid_mask.sum() > 0:
                                scaler = StandardScaler()
                                result_df.loc[valid_mask, col] = scaler.fit_transform(
                                    result_df.loc[valid_mask, [col]]
                                ).flatten()
                                applied_steps.append(f"Standardized '{col}'")
                        elif method == "minmax" and SKLEARN_AVAILABLE:
                            valid_mask = result_df[col].notna()
                            if valid_mask.sum() > 0:
                                scaler = MinMaxScaler()
                                result_df.loc[valid_mask, col] = scaler.fit_transform(
                                    result_df.loc[valid_mask, [col]]
                                ).flatten()
                                applied_steps.append(f"Applied min-max scaling to '{col}'")
                        elif method == "winsorize_1_99":
                            # Winsorization (1%/99%) - Research-based outlier handling
                            valid_values = result_df[col].dropna()
                            if len(valid_values) > 0:
                                lower_bound = float(valid_values.quantile(0.01))
                                upper_bound = float(valid_values.quantile(0.99))
                                result_df[col] = result_df[col].clip(lower=lower_bound, upper=upper_bound)
                                applied_steps.append(
                                    f"Applied winsorization (1%/99%) to '{col}' - "
                                    f"clipped to [{lower_bound:.2f}, {upper_bound:.2f}]"
                                )
                        elif method == "binary_discretize":
                            # Binary discretization (from NeurIPS 2024) - for multimodal distributions
                            valid_values = result_df[col].dropna()
                            if len(valid_values) > 0:
                                try:
                                    bins = pd.qcut(
                                        valid_values,
                                        q=min(256, len(valid_values.unique())),
                                        duplicates='drop',
                                        retbins=True
                                    )[1]
                                    result_df[col + '_binned'] = pd.cut(
                                        result_df[col],
                                        bins=bins,
                                        include_lowest=True
                                    )
                                    dummies = pd.get_dummies(result_df[col + '_binned'], prefix=col)
                                    result_df = pd.concat([
                                        result_df.drop(columns=[col, col + '_binned']),
                                        dummies
                                    ], axis=1)
                                    applied_steps.append(
                                        f"Applied binary discretization to '{col}' "
                                        f"({len(dummies.columns)} bins)"
                                    )
                                except Exception as e:
                                    logger.warning(f"Failed to apply binary discretization to '{col}': {e}")
                    except Exception as e:
                        logger.warning(f"Failed to apply transformation '{method}' to '{col}': {e}")
        
        # 7. Categorical encoding
        if "categorical_encoding" in plan and isinstance(plan["categorical_encoding"], dict):
            for col, config in plan["categorical_encoding"].items():
                if col in result_df.columns:
                    try:
                        method = config.get("method", "label")
                        if method == "one_hot":
                            dummies = pd.get_dummies(result_df[col], prefix=col)
                            result_df = pd.concat([result_df.drop(columns=[col]), dummies], axis=1)
                            applied_steps.append(f"One-hot encoded '{col}' ({len(dummies.columns)} categories)")
                        elif method == "label":
                            result_df[col] = pd.Categorical(result_df[col]).codes
                            applied_steps.append(f"Label encoded '{col}'")
                        elif method == "group_rare":
                            max_categories = config.get("max_categories", 50)
                            value_counts = result_df[col].value_counts()
                            rare_values = value_counts[value_counts < len(result_df) * 0.01].index
                            result_df[col] = result_df[col].replace(rare_values, "RARE")
                            applied_steps.append(f"Grouped rare categories in '{col}' (max {max_categories})")
                    except Exception as e:
                        logger.warning(f"Failed to encode categorical '{col}': {e}")
    
    except Exception as e:
        logger.error(f"Error applying preprocessing plan: {type(e).__name__}: {e}", exc_info=True)
        return df, {"error": str(e), "applied_steps": applied_steps}
    
    # FINAL VALIDATION: Ensure output is compatible with SynthCity GenericDataLoader
    # SynthCity requires: numeric (int/float), categorical (category), or string (object)
    try:
        for col in result_df.columns:
            col_data = result_df[col]
            if pd.api.types.is_datetime64_any_dtype(col_data):
                result_df[col] = col_data.astype('int64') / 1e9
                applied_steps.append(f"Final conversion: '{col}' datetime → numeric timestamp")
            elif pd.api.types.is_bool_dtype(col_data):
                result_df[col] = col_data.astype(int).astype('int64')
                applied_steps.append(f"Final conversion: '{col}' boolean → int")
            elif pd.api.types.is_numeric_dtype(col_data):
                # Use regular int64 (not nullable Int64) for SynthCity compatibility
                if col_data.dtype in ['int64', 'int32', 'int16', 'int8', 'Int64', 'Int32', 'Int16', 'Int8']:
                    result_df[col] = pd.to_numeric(col_data, errors='coerce').fillna(0).astype('int64')
                elif col_data.dtype not in ['float64', 'float32']:
                    result_df[col] = pd.to_numeric(col_data, errors='coerce').astype(float)
            else:
                # Ensure object columns are string or category
                if col_data.dtype != 'category':
                    if col_data.nunique() < len(col_data) * 0.5 and col_data.nunique() < 100:
                        result_df[col] = col_data.astype('category')
                    else:
                        result_df[col] = col_data.astype(str)
    except Exception as e:
        logger.warning(f"Final validation/conversion failed: {type(e).__name__}: {e}")
    
    return result_df, {"applied_steps": applied_steps, "rationale": plan.get("rationale", "")}
